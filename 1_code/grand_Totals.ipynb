{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import win32com.client\n",
    "import os\n",
    "from datetime import datetime\n",
    "import importlib\n",
    "import warnings\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 7)\n",
    "# update_files = True\n",
    "update_files = True\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Get SSO of User\n",
    "sso_user = os.path.expanduser(\"~\")[-9:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_salesforce_data(update_files=True):\n",
    "    file_address = os.path.join(\n",
    "        r\"C:/Users\",\n",
    "        sso_user,\n",
    "        \"Box\",\n",
    "        \"FieldCore Technical Learning and Development\",\n",
    "        \"Tools\",\n",
    "        \"RAW Data for Reports and Tools\",\n",
    "        \"data for pythong scripts\",\n",
    "        \"salesforce_data.xlsx\",\n",
    "    )\n",
    "\n",
    "    print(f\"Reading in file from {file_address}\")\n",
    "\n",
    "    if update_files:\n",
    "        \"\"\"Returns True if the file was refreshed today, False otherwise.\"\"\"\n",
    "        last_modified_time = os.path.getmtime(file_address)\n",
    "        today = datetime.today()\n",
    "\n",
    "        if not today.date() == datetime.fromtimestamp(last_modified_time).date():\n",
    "            print(\"Updaing Smax File\")\n",
    "            updated_smax(file_address)\n",
    "\n",
    "        else:\n",
    "            refresh_input = input(\"Do you want to refresh smax (y/n)?\")\n",
    "            if refresh_input == \"y\":\n",
    "                updated_smax(file_address)\n",
    "            else:\n",
    "                print(\"No update needed\")\n",
    "\n",
    "    if \"dfs\" in locals() or \"dfs\" in globals():\n",
    "        print(\"Salesforce data already loaded\")\n",
    "    else:\n",
    "        # read in salesforce data\n",
    "        print(\"Reading in Salesforce Data...\")\n",
    "        try:\n",
    "            excel_file = pd.ExcelFile(file_address)\n",
    "            print(\"Salesforce Data loaded...\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: file not found at '{file_address}'\")\n",
    "        except pd.errors.ParserError:\n",
    "            print(\n",
    "                f\"Error: Failed to parse the finance data CSV file at '{file_address}'\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")\n",
    "\n",
    "        sheet_names = excel_file.sheet_names\n",
    "        dfs = {sheet_name: excel_file.parse(sheet_name) for sheet_name in sheet_names}\n",
    "\n",
    "        excel_file.close()\n",
    "        print(\"...complete\")\n",
    "\n",
    "    return dfs\n",
    "\n",
    "def updated_smax(file_address):\n",
    "    # Opening Excel software using the win32com\n",
    "    File = win32com.client.Dispatch(\"Excel.Application\")\n",
    "\n",
    "    # Optional line to show the Excel software\n",
    "    File.Visible = 1\n",
    "\n",
    "    # Opening your workbook\n",
    "    print(\"Updating Smax File\")\n",
    "    Workbook = File.Workbooks.open(file_address)\n",
    "\n",
    "    # Refeshing all the shests\n",
    "    Workbook.RefreshAll()\n",
    "    File.CalculateUntilAsyncQueriesDone()\n",
    "\n",
    "    # Saving the Workbook\n",
    "    Workbook.Save()\n",
    "    Workbook.Close()\n",
    "    # Closing the Excel File\n",
    "    File.Quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in file from C:/Users\\605042670\\Box\\FieldCore Technical Learning and Development\\Tools\\RAW Data for Reports and Tools\\data for pythong scripts\\salesforce_data.xlsx\n",
      "Reading in Salesforce Data...\n",
      "Salesforce Data loaded...\n",
      "...complete\n"
     ]
    }
   ],
   "source": [
    "if 'dfs' in locals() or 'dfs' in globals():\n",
    "    print(\"Salesforce data already loaded\")\n",
    "else:\n",
    "    dfs = import_salesforce_data(update_files=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "grand_Total_df = dfs[\"Grand Total Ops\"]\n",
    "grand_Total_df.to_excel(\"C:/Users/605042670/Desktop/Github/Training_Operations_Bulletin/2_pipeline/ServiceMax_GrandTotal.xlsx\")\n",
    "destination_df = pd.read_excel(\"C:/Users/605042670/Desktop/Github/Training_Operations_Bulletin/2_pipeline/GrandTotal.xlsx\", sheet_name=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Week'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\605042670\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\605042670\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\605042670\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Week'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\605042670\\Desktop\\Github\\Training_Operations_Bulletin\\1_code\\grand_Totals.ipynb Cell 5\u001b[0m line \u001b[0;36m<cell line: 22>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/605042670/Desktop/Github/Training_Operations_Bulletin/1_code/grand_Totals.ipynb#X15sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m sheet_name, destination_df \u001b[39min\u001b[39;00m destination_sheets\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/605042670/Desktop/Github/Training_Operations_Bulletin/1_code/grand_Totals.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     \u001b[39m# Check if the sheet name exists in the 'Region' column of the source file\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/605042670/Desktop/Github/Training_Operations_Bulletin/1_code/grand_Totals.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mif\u001b[39;00m sheet_name \u001b[39min\u001b[39;00m source_df[\u001b[39m'\u001b[39m\u001b[39mRegion\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/605042670/Desktop/Github/Training_Operations_Bulletin/1_code/grand_Totals.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         \u001b[39m# Filter source data based on the 'Region' and current week number\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/605042670/Desktop/Github/Training_Operations_Bulletin/1_code/grand_Totals.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         source_data_for_sheet \u001b[39m=\u001b[39m source_df[(source_df[\u001b[39m'\u001b[39m\u001b[39mRegion\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m sheet_name) \u001b[39m&\u001b[39m (source_df[\u001b[39m'\u001b[39;49m\u001b[39mWeek\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m==\u001b[39m current_week)][[\u001b[39m'\u001b[39m\u001b[39mOps#1\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mOps#2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mOps#3\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mOps#4\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mOps#5\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mOps#6\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mOps#7\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mOps#8\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mOps#9\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mOps#10\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mOps#11\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mOps#12\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/605042670/Desktop/Github/Training_Operations_Bulletin/1_code/grand_Totals.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         \u001b[39m# Merge or append the additional data to the existing sheet data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/605042670/Desktop/Github/Training_Operations_Bulletin/1_code/grand_Totals.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         updated_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mmerge(destination_df, source_data_for_sheet, left_on\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWeek\u001b[39m\u001b[39m'\u001b[39m, right_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m, suffixes\u001b[39m=\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_source\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\605042670\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\605042670\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Week'"
     ]
    }
   ],
   "source": [
    "# Specify the Excel file path\n",
    "excel_file_path = \"C:/Users/605042670/Desktop/Github/Training_Operations_Bulletin/2_pipeline/GrandTotal.xlsx\"\n",
    "\n",
    "# Get the current week number\n",
    "current_week = datetime.now().isocalendar()[1]\n",
    "\n",
    "# Use pd.read_excel with sheet_name set to None to get the sheet names\n",
    "excel_sheets = pd.read_excel(excel_file_path, sheet_name=None)\n",
    "\n",
    "# Get the sheet names\n",
    "sheet_names = list(excel_sheets.keys())\n",
    "\n",
    "# Specify the Excel file paths\n",
    "source_file_path = \"C:/Users/605042670/Desktop/Github/Training_Operations_Bulletin/2_pipeline/ServiceMax_GrandTotal.xlsx\"\n",
    "# Read source data\n",
    "source_df = pd.read_excel(source_file_path)\n",
    "\n",
    "# Read destination data into a dictionary of DataFrames\n",
    "destination_sheets = pd.read_excel(excel_file_path, sheet_name=None)\n",
    "\n",
    "# Iterate over each sheet in the destination file\n",
    "for sheet_name, destination_df in destination_sheets.items():\n",
    "    # Check if the sheet name exists in the 'Region' column of the source file\n",
    "    if sheet_name in source_df['Region'].values:\n",
    "        # Filter source data based on the 'Region' and current week number\n",
    "        source_data_for_sheet = source_df[(source_df['Region'] == sheet_name) & (source_df['Week'] == current_week)][['Ops#1', 'Ops#2', 'Ops#3', 'Ops#4', 'Ops#5', 'Ops#6', 'Ops#7', 'Ops#8', 'Ops#9', 'Ops#10', 'Ops#11', 'Ops#12']]\n",
    "\n",
    "        # Merge or append the additional data to the existing sheet data\n",
    "        updated_df = pd.merge(destination_df, source_data_for_sheet, left_on='Week', right_index=True, how='left', suffixes=('', '_source'))\n",
    "\n",
    "        # Write the updated DataFrame to the sheet\n",
    "        updated_df.to_excel(excel_file_path, sheet_name=sheet_name, index=False)\n",
    "    else:\n",
    "        print(f\"Sheet '{sheet_name}' not found in the source file.\")\n",
    "\n",
    "print(\"Update completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Week</th>\n",
       "      <th>Ops#1</th>\n",
       "      <th>Ops#2</th>\n",
       "      <th>Ops#3</th>\n",
       "      <th>Ops#4</th>\n",
       "      <th>Ops#5</th>\n",
       "      <th>Ops#6</th>\n",
       "      <th>Ops#7</th>\n",
       "      <th>Ops#8</th>\n",
       "      <th>Ops#9</th>\n",
       "      <th>Ops#10</th>\n",
       "      <th>Ops#11</th>\n",
       "      <th>Ops#12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>week 46</td>\n",
       "      <td>4663.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>week 47</td>\n",
       "      <td>4632.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>week 48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>week 09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>week 10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>week 11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Week   Ops#1  Ops#2  Ops#3  Ops#4  Ops#5  Ops#6  Ops#7  Ops#8  Ops#9  \\\n",
       "0   week 46  4663.0   11.0    4.0    2.0    0.0  267.0   12.0    NaN    NaN   \n",
       "1   week 47  4632.0   11.0    2.0  152.0    0.0  159.0   12.0    NaN    NaN   \n",
       "2   week 48     NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "..      ...     ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "15  week 09     NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "16  week 10     NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "17  week 11     NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN    NaN   \n",
       "\n",
       "    Ops#10  Ops#11  Ops#12  \n",
       "0      0.0    70.0     0.0  \n",
       "1      0.0    70.0     0.0  \n",
       "2      NaN     NaN     NaN  \n",
       "..     ...     ...     ...  \n",
       "15     NaN     NaN     NaN  \n",
       "16     NaN     NaN     NaN  \n",
       "17     NaN     NaN     NaN  \n",
       "\n",
       "[18 rows x 13 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the week of the year\n",
    "current_date = datetime.now()\n",
    "week_number = current_date.isocalendar()[1]\n",
    "print(f\"Week of the year: {week_number}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each sheet in the source file\n",
    "for sheet_name, source_sheet in grand_Total_df.groupby('Region'):\n",
    "    # Check if the sheet exists in the destination file\n",
    "    if sheet_name in destination_df:\n",
    "        # Merge the source data into the destination data based on Week\n",
    "        merged_data = pd.merge(destination_df[sheet_name], source_sheet[['Week'] + [f'Ops#{i}' for i in range(1, 13)]], on='Week', how='outer')\n",
    "\n",
    "        # Sort the data based on Week\n",
    "        merged_data.sort_values(by='Week', inplace=True)\n",
    "\n",
    "        # Update the destination sheet with the merged and sorted data\n",
    "        destination_df[sheet_name] = merged_data\n",
    "    else:\n",
    "        print(f\"Sheet '{sheet_name}' not found in the destination file.\")\n",
    "\n",
    "# Save the updated destination data to the same Excel file\n",
    "with pd.ExcelWriter(\"C:/Users/605042670/Desktop/Github/Training_Operations_Bulletin/2_pipeline\\GrandTotal.xlsx\", engine='openpyxl') as writer:\n",
    "    for sheet_name, df in destination_df.items():\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
